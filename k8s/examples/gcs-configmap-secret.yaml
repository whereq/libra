# ==============================================================================
# GCS Configuration Example for Kubernetes
# ==============================================================================
# This file demonstrates how to configure GCS access for Spark on Kubernetes
# using ConfigMaps for configuration and Secrets for credentials.
#
# IMPORTANT: Credentials are provided at RUNTIME, NOT baked into Docker images!
#
# Usage:
#   1. Create GCS service account key JSON file
#   2. Create secret: kubectl create secret generic gcs-key --from-file=gcs-key.json=path/to/key.json -n spark-platform
#   3. Apply this file: kubectl apply -f gcs-configmap-secret.yaml
#   4. Deploy Spark master/worker with volume mounts
# ==============================================================================

---
# GCS Service Account Key Secret
# Create this using: kubectl create secret generic gcs-service-account-key \
#   --from-file=gcs-key.json=/path/to/your-service-account-key.json \
#   -n spark-platform
apiVersion: v1
kind: Secret
metadata:
  name: gcs-service-account-key
  namespace: spark-platform
type: Opaque
# data:
#   gcs-key.json: <base64-encoded-service-account-json>
#
# OR create from file:
# kubectl create secret generic gcs-service-account-key \
#   --from-file=gcs-key.json=./my-project-key.json \
#   -n spark-platform

---
# GCS Configuration ConfigMap
# Contains runtime environment-specific settings
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-gcs-config
  namespace: spark-platform
data:
  # GCS bucket for Spark data
  GCS_BUCKET: "my-spark-bucket"

  # Google Cloud Project ID
  GCS_PROJECT_ID: "my-gcp-project-id"

  # Optional: Additional Spark configurations
  spark-env-overrides.conf: |
    # Master configuration
    spark.master=spark://spark-master:7077
    spark.submit.deployMode=client
    spark.scheduler.mode=FAIR

    # GCS as default filesystem
    spark.hadoop.fs.default.name=gs://my-spark-bucket/

    # Event logging
    spark.eventLog.dir=gs://my-spark-bucket/spark_events
    spark.history.fs.logDirectory=gs://my-spark-bucket/spark_events

    # SQL warehouse
    spark.sql.warehouse.dir=gs://my-spark-bucket/spark_warehouse/

---
# Environment-specific ConfigMaps for different environments
---
# Development Environment
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-gcs-config-dev
  namespace: spark-platform
data:
  GCS_BUCKET: "dev-spark-bucket"
  GCS_PROJECT_ID: "my-project-dev"
  spark-env-overrides.conf: |
    spark.master=spark://spark-master:7077
    spark.hadoop.fs.default.name=gs://dev-spark-bucket/
    spark.eventLog.dir=gs://dev-spark-bucket/spark_events
    spark.sql.warehouse.dir=gs://dev-spark-bucket/spark_warehouse/

---
# Staging Environment
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-gcs-config-staging
  namespace: spark-platform
data:
  GCS_BUCKET: "staging-spark-bucket"
  GCS_PROJECT_ID: "my-project-staging"
  spark-env-overrides.conf: |
    spark.master=spark://spark-master:7077
    spark.hadoop.fs.default.name=gs://staging-spark-bucket/
    spark.eventLog.dir=gs://staging-spark-bucket/spark_events
    spark.sql.warehouse.dir=gs://staging-spark-bucket/spark_warehouse/

---
# Production Environment
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-gcs-config-prod
  namespace: spark-platform
data:
  GCS_BUCKET: "prod-spark-bucket"
  GCS_PROJECT_ID: "my-project-prod"
  spark-env-overrides.conf: |
    spark.master=spark://spark-master:7077
    spark.hadoop.fs.default.name=gs://prod-spark-bucket/
    spark.eventLog.dir=gs://prod-spark-bucket/spark_events
    spark.sql.warehouse.dir=gs://prod-spark-bucket/spark_warehouse/
