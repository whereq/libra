# WhereQ Libra - Resource Allocation Guide

## Overview

This guide explains how Spark resource allocation works in WhereQ Libra and how to configure driver/executor memory and cores for your jobs.

## Table of Contents

1. [Understanding Spark Resource Allocation](#understanding-spark-resource-allocation)
2. [Global vs Per-Job Configuration](#global-vs-per-job-configuration)
3. [Resource Allocation by Job Type](#resource-allocation-by-job-type)
4. [Configuration Examples](#configuration-examples)
5. [Best Practices](#best-practices)

---

## Understanding Spark Resource Allocation

### Spark Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Spark Application                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Driver     â”‚  â† Controls execution, schedules tasks    â”‚
â”‚  â”‚              â”‚  â† spark.driver.memory, spark.driver.coresâ”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚          Cluster Manager                 â”‚               â”‚
â”‚  â”‚   (local[*], YARN, K8s, Standalone)      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Executor 1  â”‚  â”‚  Executor 2  â”‚  â”‚  Executor N  â”‚       â”‚
â”‚  â”‚  - Memory    â”‚  â”‚  - Memory    â”‚  â”‚  - Memory    â”‚       â”‚
â”‚  â”‚  - Cores     â”‚  â”‚  - Cores     â”‚  â”‚  - Cores     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â†‘                  â†‘                  â†‘             â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚    spark.executor.memory   â”‚                  â”‚             â”‚
â”‚    spark.executor.cores    â”‚                  â”‚             â”‚
â”‚    spark.executor.instances (# of executors)  â”‚             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Resource Parameters

| Parameter | Description | Example | When to Increase |
|-----------|-------------|---------|------------------|
| **spark.driver.memory** | Memory for driver process | `2g`, `4g`, `8g` | Large result sets, collect(), broadcast variables |
| **spark.driver.cores** | CPU cores for driver | `1`, `2`, `4` | Complex driver computations |
| **spark.executor.memory** | Memory per executor | `2g`, `4g`, `16g` | Large datasets, caching, shuffles |
| **spark.executor.cores** | CPU cores per executor | `2`, `4`, `8` | CPU-intensive transformations |
| **spark.executor.instances** | Number of executors | `2`, `10`, `50` | Parallelism, data volume |
| **spark.dynamicAllocation.enabled** | Auto-scale executors | `true`, `false` | Variable workloads |

---

## Global vs Per-Job Configuration

### Global Configuration (application.yml)

**Location:** `src/main/resources/application.yml`

```yaml
spark:
  app-name: whereq-libra
  master: local[*]  # All available cores
  config:
    spark.driver.memory: 2g
    spark.executor.memory: 2g
    spark.sql.warehouse.dir: /tmp/spark-warehouse
    spark.scheduler.mode: FAIR
```

**When to Use:**
- âœ… Default settings for all jobs
- âœ… Development/testing environments
- âœ… Homogeneous workload patterns

### Per-Job Configuration (API Request)

**When to Use:**
- âœ… Resource-intensive specific jobs
- âœ… Different job requirements
- âœ… Production workloads with varied characteristics

**Supported via `sparkConfig` field in API request**

---

## Resource Allocation by Job Type

### 1. `jar-class` Mode (Intelligent Auto-Switching) ğŸ¯

**How It Works:**
```
Libra automatically chooses execution mode based on sparkConfig:

WITHOUT resource config:          WITH resource config:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  In-JVM Execution    â”‚         â”‚  Auto-switch to      â”‚
â”‚  (Shared Session)    â”‚         â”‚  spark-submit mode   â”‚
â”‚                      â”‚         â”‚  (Dedicated Resourcesâ”‚
â”‚  Fast & Efficient    â”‚         â”‚  â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Resource Behavior:**
- âœ… **Smart mode switching** based on sparkConfig
- âœ… **NO resource config** â†’ Uses shared SparkSession (fast)
- âœ… **WITH resource config** â†’ Auto-switches to spark-submit (custom resources)
- âœ… Transparent to the caller
- â„¹ï¸  Logs warning when auto-switching occurs

**When to Use:**
- Want simplicity - Libra decides the best execution method
- Small jobs without sparkConfig â†’ Fast in-JVM
- Large jobs with sparkConfig â†’ Dedicated resources

**Example 1: Fast In-JVM (No Resources Specified)**
```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "jar-class",
    "filePath": "my-app.jar",
    "mainClass": "com.example.MyApp"
  }'
# â†’ Executes in-JVM, uses shared session (2g default)
```

**Example 2: Auto-Switch to spark-submit (Resources Specified)**
```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "jar-class",
    "filePath": "my-app.jar",
    "mainClass": "com.example.MyApp",
    "sparkConfig": {
      "spark.driver.memory": "4g",
      "spark.executor.memory": "8g"
    }
  }'
# â†’ Auto-switches to spark-submit with 4g driver, 8g executors
# â†’ Logs: "Automatically switching to 'jar' mode..."
```

---

### 2. `jar` Mode (spark-submit) âœ…

**How It Works:**
```
Libra Process                    Separate JVM Process
â”œâ”€â”€ Receives request             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”œâ”€â”€ Calls spark-submit â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ Your Spark App       â”‚
â””â”€â”€ Monitors execution           â”‚ â”œâ”€â”€ SparkSession     â”‚
                                 â”‚ â”œâ”€â”€ Driver           â”‚
                                 â”‚ â””â”€â”€ Executors        â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 (NEW resources allocated)
```

**Resource Behavior:**
- âœ… **FULL control over resources per-job**
- âœ… Creates separate JVM with specified resources
- âœ… Complete isolation from other jobs
- âœ… `sparkConfig` fully supported
- âš ï¸  Slower startup (JVM + SparkContext initialization)

**When to Use:**
- Large jobs requiring significant resources
- Jobs with different resource profiles
- Complete isolation required
- Production deployments with SLAs

**Example:**
```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "jar",
    "filePath": "my-big-job.jar",
    "mainClass": "com.example.BigDataJob",
    "sparkConfig": {
      "spark.driver.memory": "4g",
      "spark.driver.cores": "2",
      "spark.executor.memory": "8g",
      "spark.executor.cores": "4",
      "spark.executor.instances": "10",
      "spark.dynamicAllocation.enabled": "true"
    }
  }'
```

---

### 3. `python` and `python-file` Modes âœ…

**How It Works:**
```
Python jobs execute via spark-submit:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  spark-submit script.py         â”‚
â”‚  - Separate process             â”‚
â”‚  - Full resource control        â”‚
â”‚  - Per-job configuration        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Resource Behavior:**
- âœ… **FULL control over resources per-job**
- âœ… Executes via spark-submit (separate process)
- âœ… Supports all resource configuration options
- âœ… Per-job driver/executor memory and cores

**When to Use:**
- Python-based data processing
- ML pipelines with PySpark
- ETL jobs in Python

**Example: Python with Custom Resources**
```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "python-file",
    "filePath": "/path/to/etl_job.py",
    "args": ["2025-11-03", "/data/input"],
    "sparkConfig": {
      "spark.driver.memory": "4g",
      "spark.executor.memory": "8g",
      "spark.executor.cores": "4",
      "spark.executor.instances": "10"
    }
  }'
```

---

### 4. `sql` Mode (In-JVM) âš ï¸

**Resource Behavior:**
- âŒ **CANNOT change driver/executor resources**
- âœ… Uses shared SparkSession
- âœ… Can set job-level SQL configs

**Supported Per-Job Configs:**
```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "sql",
    "code": "SELECT * FROM large_table",
    "sparkConfig": {
      "spark.sql.shuffle.partitions": "400",       # âœ… Works
      "spark.sql.adaptive.enabled": "true",        # âœ… Works
      "spark.sql.adaptive.coalescePartitions.enabled": "true",  # âœ… Works
      "spark.executor.memory": "8g"                # âŒ Ignored (shared session)
    }
  }'
```

---

## Configuration Examples

### Example 1: Small ETL Job (jar-class)

**Scenario:** Quick data transformation, small dataset

```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "jar-class",
    "filePath": "etl-small.jar",
    "mainClass": "com.company.etl.SmallETL",
    "args": ["2025-11-03", "/data/input", "/data/output"],
    "pool": "interactive"
  }'
```

**Resource Allocation:**
- Uses global config (2g driver, 2g executor)
- Fast execution (no JVM startup)

---

### Example 2: Large ML Training Job (jar)

**Scenario:** Machine learning model training, 100GB dataset

```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "jar",
    "filePath": "ml-training.jar",
    "mainClass": "com.company.ml.TrainModel",
    "args": ["/data/features", "/models/output"],
    "pool": "high-priority",
    "sparkConfig": {
      "spark.driver.memory": "8g",
      "spark.driver.cores": "4",
      "spark.executor.memory": "16g",
      "spark.executor.cores": "8",
      "spark.executor.instances": "20",
      "spark.sql.adaptive.enabled": "true",
      "spark.sql.adaptive.coalescePartitions.enabled": "true"
    }
  }'
```

**Resource Allocation:**
- Driver: 8GB RAM, 4 CPU cores
- Executors: 20 executors Ã— 16GB Ã— 8 cores = 320GB total
- Adaptive execution enabled for optimization

---

### Example 3: Variable Workload (jar with dynamic allocation)

**Scenario:** Unpredictable data volume, need auto-scaling

```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "jar",
    "filePath": "batch-processor.jar",
    "mainClass": "com.company.batch.Processor",
    "sparkConfig": {
      "spark.driver.memory": "4g",
      "spark.executor.memory": "8g",
      "spark.executor.cores": "4",
      "spark.dynamicAllocation.enabled": "true",
      "spark.dynamicAllocation.minExecutors": "2",
      "spark.dynamicAllocation.maxExecutors": "50",
      "spark.dynamicAllocation.initialExecutors": "5"
    }
  }'
```

**Resource Allocation:**
- Starts with 5 executors
- Scales down to minimum 2 when idle
- Scales up to maximum 50 under load

---

### Example 4: SQL Query with Optimizations

**Scenario:** Complex SQL on large table

```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "sql",
    "code": "SELECT dept, COUNT(*), AVG(salary) FROM employees GROUP BY dept",
    "pool": "default",
    "sparkConfig": {
      "spark.sql.shuffle.partitions": "800",
      "spark.sql.adaptive.enabled": "true",
      "spark.sql.adaptive.coalescePartitions.enabled": "true",
      "spark.sql.adaptive.skewJoin.enabled": "true"
    }
  }'
```

---

## Best Practices

### 1. **Right-Size Your Resources**

**Driver Memory:**
- Start with 2-4GB
- Increase if: collect(), large broadcast variables, OOM errors
- Don't over-allocate (wastes resources)

**Executor Memory:**
- Rule of thumb: `executor_memory = (dataset_size / num_executors) Ã— 1.5`
- Leave 10-20% overhead for Spark internals
- Monitor GC time (should be < 10% of task time)

**Executor Cores:**
- Recommended: 4-8 cores per executor
- More cores â†’ better parallelism
- Too many cores â†’ memory pressure per core

### 2. **Choose the Right Mode**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job Characteristic  â”‚ jar-class*  â”‚    jar     â”‚   python     â”‚ Recommend â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Small dataset       â”‚     Y       â”‚     Y      â”‚     Y        â”‚ jar-class â”‚
â”‚ Large dataset       â”‚     Y**     â”‚     Y      â”‚     Y        â”‚ jar/pythonâ”‚
â”‚ Fast turnaround     â”‚     Y       â”‚     X      â”‚     X        â”‚ jar-class â”‚
â”‚ Resource isolation  â”‚     Y**     â”‚     Y      â”‚     Y        â”‚ jar/pythonâ”‚
â”‚ Custom resources    â”‚     Y**     â”‚     Y      â”‚     Y        â”‚ Any       â”‚
â”‚ Frequent jobs       â”‚     Y       â”‚     X      â”‚     X        â”‚ jar-class â”‚
â”‚ Production SLA      â”‚     Y**     â”‚     Y      â”‚     Y        â”‚ jar/pythonâ”‚
â”‚ Python-based        â”‚     N/A     â”‚     N/A    â”‚     Y        â”‚ python    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

*  jar-class intelligently switches to spark-submit when resources specified
** jar-class with sparkConfig auto-switches to isolated spark-submit mode
```

### 3. **Monitor and Tune**

**Key Metrics to Watch:**
- Task execution time (should be 100ms - 1s)
- GC time (should be < 10% of task time)
- Shuffle read/write size
- Memory usage per executor
- Number of tasks vs parallelism

**Access Spark UI:**
```bash
# View active job metrics
http://localhost:4040

# Job history (if configured)
http://localhost:18080
```

### 4. **Local[*] Mode Considerations**

In `local[*]` mode (default in Libra):
- Driver and executors run in same JVM
- `spark.executor.memory` controls executor threads
- `spark.driver.memory` controls driver process
- Limited by host machine resources

**Example:**
```yaml
spark:
  master: local[*]  # Uses all CPU cores
  config:
    spark.driver.memory: 4g
    spark.executor.memory: 4g
```

Host with 8 cores, 16GB RAM:
- Total Spark memory: ~8GB (4g driver + 4g executors)
- Parallelism: 8 concurrent tasks

### 5. **Cluster Mode Considerations**

When deploying to YARN, Kubernetes, or Standalone:

```yaml
# application-k8s.yml
spark:
  master: k8s://https://kubernetes.default.svc:443
  config:
    spark.driver.memory: 4g
    spark.executor.memory: 8g
    spark.executor.instances: 10
    spark.kubernetes.namespace: spark-apps
    spark.kubernetes.container.image: my-spark:latest
```

**Per-job override example:**
```bash
curl -X POST http://localhost:8080/api/v1/sessions/default/statements \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "jar",
    "filePath": "hdfs://namenode:9000/apps/my-job.jar",
    "mainClass": "com.example.MyJob",
    "sparkConfig": {
      "spark.executor.memory": "16g",
      "spark.executor.instances": "50",
      "spark.kubernetes.driver.request.cores": "4",
      "spark.kubernetes.executor.request.cores": "8"
    }
  }'
```

---

## How Spark Allocates Resources

### Allocation Flow

1. **Request Submitted**
   ```
   User â†’ Libra API â†’ SparkSessionService â†’ JarJobExecutor
   ```

2. **spark-submit Command Built**
   ```bash
   spark-submit \
     --master local[*] \
     --driver-memory 4g \
     --driver-cores 2 \
     --executor-memory 8g \
     --executor-cores 4 \
     --num-executors 10 \
     --conf spark.dynamicAllocation.enabled=true \
     --class com.example.MyApp \
     my-app.jar
   ```

3. **Spark Allocates Resources**
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Cluster Manager (local, YARN, K8s)          â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                                             â”‚
   â”‚  1. Reserve driver resources (4g, 2 cores)  â”‚
   â”‚  2. Request executor containers             â”‚
   â”‚     - 10 executors Ã— 8g Ã— 4 cores           â”‚
   â”‚  3. Monitor resource availability           â”‚
   â”‚  4. Scale executors (if dynamic enabled)    â”‚
   â”‚                                             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

4. **Job Executes**
   - Driver schedules tasks based on DAG
   - Executors execute tasks in parallel
   - Shuffle data between executors as needed
   - Resources released when job completes

### Resource Contention

**In SHARED mode (jar-class, sql, python):**
```
Single SparkSession
â”œâ”€â”€ Job 1 (pool: default)     â†’ 40% resources
â”œâ”€â”€ Job 2 (pool: high-priority) â†’ 50% resources
â””â”€â”€ Job 3 (pool: low-priority)  â†’ 10% resources
     â†‘
     Controlled by fairscheduler.xml
```

**In jar mode:**
```
Job 1: Separate JVM (4g driver, 10Ã—8g executors)
Job 2: Separate JVM (8g driver, 20Ã—16g executors)
Job 3: Separate JVM (2g driver, 5Ã—4g executors)
     â†‘
     Each has dedicated resources, no contention
```

---

## Summary

### Quick Reference

| Need | Mode | Configuration Method |
|------|------|---------------------|
| Default resources for all jobs | Any | Global config (application.yml) |
| Per-job custom resources (Java) | `jar` or `jar-class`* | Per-job `sparkConfig` field |
| Per-job custom resources (Python) | `python` or `python-file` | Per-job `sparkConfig` field |
| SQL-specific tuning | `sql` | Per-job `sparkConfig` (limited) |
| Fast execution, shared resources | `jar-class` (no sparkConfig) | Global config only |
| Large isolated jobs | `jar` | Per-job `sparkConfig` |
| Intelligent mode switching | `jar-class` (with sparkConfig) | Auto-switches to spark-submit |

*`jar-class` with `sparkConfig` automatically switches to spark-submit mode

### Key Takeaways

1. âœ… **jar mode** = Full per-job resource control (always spark-submit)
2. âœ… **jar-class mode** = Smart auto-switching (in-JVM or spark-submit)
3. âœ… **python/python-file modes** = Full per-job resource control
4. âš ï¸  **sql mode** = Shared resources, limited configs
5. ğŸ“Š Monitor Spark UI (port 4040) for tuning insights
6. ğŸ¯ Start conservative, scale up based on metrics
7. ğŸ”„ Use dynamic allocation for variable workloads

---

**Author:** WhereQ Inc.
**Updated:** 2025-11-03
