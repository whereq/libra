package com.whereq.libra.queue;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.whereq.libra.dto.SparkJobRequest;
import com.whereq.libra.model.QueuedJob;
import com.whereq.libra.model.ResourceRequirement;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.domain.Range;
import org.springframework.data.redis.connection.stream.*;
import org.springframework.data.redis.core.ReactiveRedisTemplate;
import org.springframework.data.redis.stream.StreamReceiver;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import jakarta.annotation.PostConstruct;
import java.net.InetAddress;
import java.net.UnknownHostException;
import java.time.Duration;
import java.time.Instant;
import java.util.Map;
import java.util.UUID;

/**
 * Redis Streams based job queue implementation
 */
@Slf4j
@Service
public class RedisJobQueue implements JobQueue {

    private static final String STREAM_KEY = "libra:jobs:pending";
    private static final String CONSUMER_GROUP = "libra-processors";
    private final String consumerName;

    @Autowired
    private ReactiveRedisTemplate<String, String> redisTemplate;

    @Autowired
    private ObjectMapper objectMapper;

    public RedisJobQueue() {
        this.consumerName = generateConsumerName();
    }

    @PostConstruct
    public void initialize() {
        // Create consumer group if not exists
        redisTemplate.opsForStream()
            .createGroup(STREAM_KEY, ReadOffset.from("0"), CONSUMER_GROUP)
            .doOnSuccess(s -> log.info("Created consumer group: {}", CONSUMER_GROUP))
            .onErrorResume(e -> {
                log.info("Consumer group already exists: {}", CONSUMER_GROUP);
                return Mono.empty();
            })
            .subscribe();
    }

    @Override
    public Mono<Void> enqueue(QueuedJob job) {
        return Mono.fromCallable(() -> {
            try {
                Map<String, String> record = Map.of(
                    "jobId", job.getJobId(),
                    "userId", job.getUserId(),
                    "payload", objectMapper.writeValueAsString(job.getRequest()),
                    "resourceRequirement", objectMapper.writeValueAsString(job.getResourceRequirement()),
                    "retryCount", String.valueOf(job.getRetryCount()),
                    "enqueuedAt", Instant.now().toString()
                );

                return record;
            } catch (JsonProcessingException e) {
                throw new RuntimeException("Failed to serialize job", e);
            }
        })
        .flatMap(record -> redisTemplate.opsForStream().add(STREAM_KEY, record))
        .doOnSuccess(recordId -> log.info("Enqueued job {} with recordId {}", job.getJobId(), recordId))
        .then()
        .subscribeOn(Schedulers.boundedElastic());
    }

    @Override
    public Flux<QueuedJob> consumeAsFlux() {
        // Simplified consumption using ReactiveRedisTemplate directly
        Consumer consumer = Consumer.from(CONSUMER_GROUP, consumerName);
        StreamOffset<String> streamOffset = StreamOffset.create(STREAM_KEY, ReadOffset.lastConsumed());

        StreamReadOptions readOptions = StreamReadOptions.empty()
            .count(1)
            .block(Duration.ofSeconds(5));

        return Flux.interval(Duration.ofSeconds(1))
            .flatMap(tick -> redisTemplate.opsForStream()
                .read(consumer, readOptions, streamOffset)
                .flatMapIterable(record -> record)
                .map(this::deserializeJob)
                .doOnNext(job -> log.debug("Consumed job {} from queue", job.getJobId()))
            )
            .subscribeOn(Schedulers.boundedElastic());
    }

    @Override
    public Mono<Void> acknowledge(String jobId) {
        return findRecordId(jobId)
            .flatMap(recordId -> redisTemplate.opsForStream()
                .acknowledge(STREAM_KEY, CONSUMER_GROUP, recordId)
                .doOnSuccess(ack -> log.info("Acknowledged job {} (recordId: {})", jobId, recordId))
                .then()
            )
            .switchIfEmpty(Mono.fromRunnable(() ->
                log.warn("No record found to acknowledge for job {}", jobId)
            ));
    }

    @Override
    public Mono<Void> remove(String jobId) {
        return findRecordId(jobId)
            .flatMap(recordId -> redisTemplate.opsForStream()
                .delete(STREAM_KEY, recordId)
                .doOnSuccess(deleted -> log.info("Removed job {} from queue (deleted: {})", jobId, deleted))
                .then()
            )
            .switchIfEmpty(Mono.fromRunnable(() ->
                log.warn("No record found to remove for job {}", jobId)
            ));
    }

    @Override
    public Mono<Long> size() {
        return redisTemplate.opsForStream()
            .size(STREAM_KEY)
            .defaultIfEmpty(0L);
    }

    private QueuedJob deserializeJob(MapRecord<String, String, String> record) {
        try {
            Map<String, String> value = record.getValue();

            String jobId = value.get("jobId");
            String userId = value.get("userId");
            SparkJobRequest request = objectMapper.readValue(
                value.get("payload"),
                SparkJobRequest.class
            );
            ResourceRequirement resourceRequirement = objectMapper.readValue(
                value.get("resourceRequirement"),
                ResourceRequirement.class
            );
            int retryCount = Integer.parseInt(value.getOrDefault("retryCount", "0"));
            Instant enqueuedAt = Instant.parse(value.get("enqueuedAt"));

            return QueuedJob.builder()
                .jobId(jobId)
                .userId(userId)
                .request(request)
                .resourceRequirement(resourceRequirement)
                .recordId(record.getId().getValue())
                .enqueuedAt(enqueuedAt)
                .retryCount(retryCount)
                .build();

        } catch (JsonProcessingException e) {
            throw new RuntimeException("Failed to deserialize job from record: " + record.getId(), e);
        }
    }

    private Mono<String> findRecordId(String jobId) {
        // Query pending messages for this consumer group to find the recordId
        return redisTemplate.opsForStream()
            .pending(STREAM_KEY, CONSUMER_GROUP, Range.unbounded(), Long.MAX_VALUE)
            .flatMap(pendingMessage -> {
                // Read the actual message to check jobId
                return redisTemplate.opsForStream()
                    .range(STREAM_KEY, Range.closed(pendingMessage.getIdAsString(), pendingMessage.getIdAsString()))
                    .next()
                    .filter(record -> jobId.equals(record.getValue().get("jobId")))
                    .map(record -> record.getId().getValue());
            })
            .next();
    }

    private static String generateConsumerName() {
        try {
            return InetAddress.getLocalHost().getHostName() + "-" +
                   ProcessHandle.current().pid();
        } catch (UnknownHostException e) {
            return "libra-" + UUID.randomUUID().toString();
        }
    }
}
